{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Used Source: https://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "# http://tweepy.readthedocs.io/en/v3.5.0/index.html\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "# https://pandasguide.readthedocs.io/en/latest/\n",
    "import pandas as pd\n",
    "# https://numpy.readthedocs.io/en/latest/\n",
    "import numpy as np\n",
    "# https://api.mongodb.com/python/current/\n",
    "import pymongo\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "\n",
    "#nltk data for tokenizing and stoppword removal\n",
    "nltk.download('stopwords')\n",
    "nltk.download('subjectivity')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "\n",
    "with open('./files/train_data.csv','rt') as csv_data:\n",
    "    reader = csv.reader(csv_data, delimiter=';')\n",
    "    for labeled_tweet in reader:\n",
    "        if labeled_tweet[1] == '0':\n",
    "            labeled_tweet[1] = 'negative'\n",
    "        else:\n",
    "            labeled_tweet[1] = 'positive'\n",
    "        train_data.append(labeled_tweet)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tweet and remove links and other unwanted information\n",
    "def clean_tweet(tweet):\n",
    "    return ''.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cln = [[clean_tweet(labeled_tweet[0]),labeled_tweet[1]] for labeled_tweet in train_data]\n",
    "tokenized_train_data = [(nltk.tokenize.word_tokenize(labeled_tweet[0]),labeled_tweet[1].lower()) for labeled_tweet in train_data_cln]\n",
    "\n",
    "print(tokenized_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "for (tweet,_) in tokenized_train_data:\n",
    "    for word in tweet:\n",
    "        if word in stopwords:\n",
    "            tweet.remove(word)\n",
    "    \n",
    "print(tokenized_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    #word_features = wordlist.most_common()\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "\n",
    "word_features = get_word_features(get_words_in_tweets(tokenized_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = nltk.classify.apply_features(extract_features, tokenized_train_data)\n",
    "print(training_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifier (~5-10 min.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETT (Estimated Time of Training) = ~ 5-10 min. => depending on Hardware and Docker settings\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_student1 = 'I hate FH JOANNEUM'\n",
    "tweet_student2 = 'FH JOANNEUM is awesome'\n",
    "\n",
    "print(\"Student1's sentiment:\", classifier.classify(extract_features(tweet_student1.split())))\n",
    "print(\"Student2's sentiment:\", classifier.classify(extract_features(tweet_student2.split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Classifier with real world Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the received credentials for your recently created TwitterAPI\n",
    "CONSUMER_KEY = 'MmiELrtF7fSp3vptCID8jKril'\n",
    "CONSUMER_SECRET = 'HqtMRk4jpt30uwDOLz30jHqZm6TPN6rj3oHFaL6xFxw2k0GkDC'\n",
    "ACCESS_TOKEN = '116725830-rkT63AILxR4fpf4kUXd8xJoOcHTsGkKUOKSMpMJQ'\n",
    "ACCESS_TOKEN_SECRET = 'eKzxfku4GdYu1wWcMr5iusTmhFT35cDWezMU2Olr5UD4i'\n",
    "\n",
    "# auth with your provided \n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "# Create an instance for the TwitterApi\n",
    "twitter = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCREEN_NAME = \"realDonaldTrump\"\n",
    "COUNT = 500\n",
    "\n",
    "#extract tweets from a user's timeline\n",
    "tweets = twitter.user_timeline(screen_name=SCREEN_NAME, count=COUNT)\n",
    "print(\"{} tweets extracted.\\n\".format(len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tweet and remove links and other unwanted information\n",
    "def clean_tweet(tweet):\n",
    "    return ''.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet))\n",
    "\n",
    "\n",
    "tweets = [clean_tweet(tweet.text) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas DataFrame out of the tweets\n",
    "data = pd.DataFrame(data=[t for t in tweets], columns=['Tweets'])\n",
    "\n",
    "# Diplay the first 5 elements of the DataFrame\n",
    "display(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets = [nltk.tokenize.word_tokenize(tweet.lower()) for tweet in tweets]\n",
    "print(tokenized_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "for tweet in tokenized_tweets:\n",
    "    for word in tweet:\n",
    "        if word in stopwords:\n",
    "            tweet.remove(word)\n",
    "\n",
    "print(tokenized_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweet = tokenized_tweets[90]\n",
    "print(tweets[90])\n",
    "print(\"Trump's Sentiment about the Tweet:\", classifier.classify(extract_features(trump_tweet)).upper())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
