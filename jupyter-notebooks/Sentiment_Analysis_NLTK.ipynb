{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Used Source: https://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import re\n",
    "# http://tweepy.readthedocs.io/en/v3.5.0/index.html\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "# https://pandasguide.readthedocs.io/en/latest/\n",
    "import pandas as pd\n",
    "# https://numpy.readthedocs.io/en/latest/\n",
    "import numpy as np\n",
    "# https://api.mongodb.com/python/current/\n",
    "import pymongo\n",
    "# Helps to save trained classifier to a file and load again\n",
    "import pickle\n",
    "# Work with csv files\n",
    "import csv\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Train Data\n",
    "### Open Data Source and format it properly for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "# we change the initial labels of 0 => negative and 1 => positive. Every tweet is therefore labeled properly\n",
    "with open('./files/tweets/train_data_32k_tweets.csv','rt') as csv_data:\n",
    "    reader = csv.reader(csv_data, delimiter=';')\n",
    "    for labeled_tweet in reader:\n",
    "        if labeled_tweet[1] == '0':\n",
    "            labeled_tweet[1] = 'negative'\n",
    "        else:\n",
    "            labeled_tweet[1] = 'positive'\n",
    "        training_data.append(labeled_tweet)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Helper Functions for Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tweet and remove links and other unwanted information\n",
    "def clean_tweet(tweet):\n",
    "    return ''.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet))\n",
    "\n",
    "\n",
    "# Tokenize every single tweet (=break down tweet into array of words with label)\n",
    "def tokenize_tweet(tweet):\n",
    "    return nltk.tokenize.word_tokenize(tweet)\n",
    "\n",
    "\n",
    "# remove common english stopwords from a tweet (i.e. 'and', 'this', 'or', 'i')\n",
    "def remove_stopwords(tweet):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    for word in tweet:\n",
    "        if word in stopwords:\n",
    "            tweet.remove(word)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean, Tokenize and Remove Stopwords from Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cleaning every tweet and removing unwanted characters like commas, links, @ signs and so on...\n",
    "We also move the actual tweet at index 0 in the array and the label (neg or pos) at index 1.\n",
    "So we have an array of arrays where each array is one tweet with the associated label => [ [tweet, label] ]'''\n",
    "cleaned_tweets = [[clean_tweet(labeled_tweet[0]),labeled_tweet[1]] for labeled_tweet in training_data]\n",
    "\n",
    "# tokenizing every tweet\n",
    "# now we change the array of every tweet to a tuple of (tweet, label)\n",
    "tokenized_clnd_tweets = [(tokenize_tweet(labeled_tweet[0]),labeled_tweet[1].lower()) for labeled_tweet in cleaned_tweets]\n",
    "\n",
    "# removing stopwords\n",
    "for (tweet,_) in tokenized_clnd_tweets:\n",
    "    remove_stopwords(tweet)\n",
    "    \n",
    "formatted_train_data = tokenized_clnd_tweets    \n",
    "# printing an example to show how the data looks like now\n",
    "print(formatted_train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Feature Extractor Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all words from all tweets and save it in an array of words\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "# use NLTK to all the features (i.e. words) from every tweet. \n",
    "# Features are important for the classifier to classify tweets using these features\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "# This mehtod is used by the classifier to extract the features from every tweet and use it for training.\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Word Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all words from all tweets first and then get all the word features and save them under the variable\n",
    "word_features = get_word_features(get_words_in_tweets(formatted_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Maximum 'size' allowed is: %s\" % len(formatted_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set the size of the training data to use for your classifier.\n",
    "Be warned: Bigger datasets will take your classifier more time but will train it far more better.\n",
    "However, if you choose a smaller dataset, the classifier will finish earlier with training but will be not that good'''\n",
    "size = 500\n",
    "\n",
    "# Extract all features from the data to prepare it for the classifier and start training it\n",
    "training_set = nltk.classify.apply_features(extract_features, formatted_train_data[:size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETT (Estimated Time of Training) = depending on size of training set and Docker config aswell as Hardware\n",
    "# Skip if trained classifier is available as saved file => go to 'Load classifier from file'\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_file = open('./files/sentiment_clf.pickle', 'wb')\n",
    "pickle.dump(classifier, classifier_file)\n",
    "classifier_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved Classifier from file (skips training if trained classifier already saved as a file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_file = open('./files/sentiment_classifier.pickle', 'rb')\n",
    "# save trained classifier (file) under classifier variable\n",
    "classifier = pickle.load(classifier_file)\n",
    "classifier_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_student1 = 'I hate FH JOANNEUM'\n",
    "tweet_student2 = 'FH JOANNEUM is awesome'\n",
    "\n",
    "print(\"Student1's sentiment:\", classifier.classify(extract_features(tweet_student1.split())).upper())\n",
    "print(\"Student2's sentiment:\", classifier.classify(extract_features(tweet_student2.split())).upper())\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Classifier with real world Tweets\n",
    "### Set-up Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the received credentials for your recently created TwitterAPI\n",
    "CONSUMER_KEY = 'MmiELrtF7fSp3vptCID8jKril'\n",
    "CONSUMER_SECRET = 'HqtMRk4jpt30uwDOLz30jHqZm6TPN6rj3oHFaL6xFxw2k0GkDC'\n",
    "ACCESS_TOKEN = '116725830-rkT63AILxR4fpf4kUXd8xJoOcHTsGkKUOKSMpMJQ'\n",
    "ACCESS_TOKEN_SECRET = 'eKzxfku4GdYu1wWcMr5iusTmhFT35cDWezMU2Olr5UD4i'\n",
    "\n",
    "# auth with your provided \n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "# Create an instance for the TwitterApi\n",
    "twitter = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Twitter configuration and account to load Tweets from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCREEN_NAME = \"realDonaldTrump\"\n",
    "COUNT = 500\n",
    "\n",
    "#extract tweets from a user's timeline\n",
    "tweets = twitter.user_timeline(screen_name=SCREEN_NAME, count=COUNT)\n",
    "print(\"%s tweets extracted\" % len(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unwanted characters from Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [clean_tweet(tweet.text) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize first few Tweets in a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas DataFrame out of the tweets\n",
    "data = pd.DataFrame(data=[t for t in tweets], columns=['Tweets'])\n",
    "\n",
    "# Diplay the first 5 elements of the DataFrame\n",
    "display(data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Tweets and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize tweets\n",
    "tokenized_tweets = [nltk.tokenize.word_tokenize(tweet.lower()) for tweet in tweets]\n",
    "\n",
    "# remove stopwords\n",
    "for tweet in tokenized_tweets:\n",
    "    remove_stopwords(tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Classifier with one of the fetched Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change tweet number to analyse different tweets that where fetched\n",
    "tweet_nr = 66\n",
    "\n",
    "trump_tweet = tokenized_tweets[tweet_nr]\n",
    "print(tweets[tweet_nr])\n",
    "print(\"Trump's Sentiment about the Tweet:\", classifier.classify(extract_features(trump_tweet)).upper())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
