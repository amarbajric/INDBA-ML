{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Used Source: https://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import re\n",
    "# http://tweepy.readthedocs.io/en/v3.5.0/index.html\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "# https://pandasguide.readthedocs.io/en/latest/\n",
    "import pandas as pd\n",
    "# https://numpy.readthedocs.io/en/latest/\n",
    "import numpy as np\n",
    "# https://api.mongodb.com/python/current/\n",
    "import pymongo\n",
    "# Helps to save trained classifier to a file and load again\n",
    "import pickle\n",
    "# Work with csv files\n",
    "import csv\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "import pymongo\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Train Data\n",
    "### Open Data Source and format it properly for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "# we change the initial labels of 0 => negative and 1 => positive. Every tweet is therefore labeled properly\n",
    "with open('./files/tweets/train_data_32k_tweets.csv','rt') as csv_data:\n",
    "    reader = csv.reader(csv_data, delimiter=';')\n",
    "    for labeled_tweet in reader:\n",
    "        if labeled_tweet[1] == '0':\n",
    "            labeled_tweet[1] = 'negative'\n",
    "        else:\n",
    "            labeled_tweet[1] = 'positive'\n",
    "        training_data.append(labeled_tweet)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Helper Functions for Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tweet and remove links and other unwanted information\n",
    "def clean_tweet(tweet):\n",
    "    return ''.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet))\n",
    "\n",
    "\n",
    "# Tokenize every single tweet (=break down tweet into array of words with label)\n",
    "def tokenize_tweet(tweet):\n",
    "    return nltk.tokenize.word_tokenize(tweet)\n",
    "\n",
    "\n",
    "# remove common english stopwords from a tweet (i.e. 'and', 'this', 'or', 'i')\n",
    "def remove_stopwords(tweet):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    for word in tweet:\n",
    "        if word in stopwords:\n",
    "            tweet.remove(word)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean, Tokenize and Remove Stopwords from Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cleaning every tweet and removing unwanted characters like commas, links, @ signs and so on...\n",
    "We also move the actual tweet at index 0 in the array and the label (neg or pos) at index 1.\n",
    "So we have an array of arrays where each array is one tweet with the associated label => [ [tweet, label] ]'''\n",
    "cleaned_tweets = [[clean_tweet(labeled_tweet[0]),labeled_tweet[1]] for labeled_tweet in training_data]\n",
    "\n",
    "# tokenizing every tweet\n",
    "# now we change the array of every tweet to a tuple of (tweet, label)\n",
    "tokenized_clnd_tweets = [(tokenize_tweet(labeled_tweet[0]),labeled_tweet[1].lower()) for labeled_tweet in cleaned_tweets]\n",
    "\n",
    "# removing stopwords\n",
    "for (tweet,_) in tokenized_clnd_tweets:\n",
    "    remove_stopwords(tweet)\n",
    "    \n",
    "formatted_train_data = tokenized_clnd_tweets    \n",
    "# printing an example to show how the data looks like now\n",
    "print(formatted_train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Feature Extractor Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all words from all tweets and save it in an array of words\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "# use NLTK to all the features (i.e. words) from every tweet. \n",
    "# Features are important for the classifier to classify tweets using these features\n",
    "def get_word_features(wordlist):\n",
    "    wordlist_freq = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist_freq.keys()\n",
    "    return word_features\n",
    "\n",
    "# This mehtod is used by the classifier to extract the features from every tweet and use it for training.\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Word Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all words from all tweets first and then get all the word features and save them under the variable\n",
    "word_features = get_word_features(get_words_in_tweets(formatted_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Maximum 'size' allowed is: %s\" % len(formatted_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set the size of the training data to use for your classifier.\n",
    "Be warned: Bigger datasets will take your classifier more time but will train it far more better.\n",
    "However, if you choose a smaller dataset, the classifier will finish earlier with training but will be not that good'''\n",
    "size = 500\n",
    "\n",
    "# Extract all features from the data to prepare it for the classifier and start training it\n",
    "training_set = nltk.classify.apply_features(extract_features, formatted_train_data[:size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETT (Estimated Time of Training) = depending on size of training set and Docker config aswell as Hardware\n",
    "# Skip if trained classifier is available as saved file => go to 'Load classifier from file'\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_file = open('./files/sentiment_clf.pickle', 'wb')\n",
    "pickle.dump(classifier, classifier_file)\n",
    "classifier_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved Classifier from file (skips training if trained classifier already saved as a file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_file = open('./files/sentiment_classifier.pickle', 'rb')\n",
    "# save trained classifier (file) under classifier variable\n",
    "classifier = pickle.load(classifier_file)\n",
    "classifier_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_student1 = 'I hate FH JOANNEUM'\n",
    "tweet_student2 = 'FH JOANNEUM is awesome'\n",
    "\n",
    "print(\"Student1's sentiment:\", classifier.classify(extract_features(tweet_student1.split())).upper())\n",
    "print(\"Student2's sentiment:\", classifier.classify(extract_features(tweet_student2.split())).upper())\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show WordCloud of most informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words = 50\n",
    "wordcloud_data = [re.match(r'^.*contains\\((.*)\\).*$', word[0], re.M).group(1) for word in classifier.most_informative_features(number_of_words)]\n",
    "\n",
    "def wordcloud_draw(data, color = 'white'):\n",
    "    text = ' '.join([word for word in data])\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,\n",
    "                      background_color=color,\n",
    "                      width=2500,\n",
    "                      height=2000).generate(text)\n",
    "    plt.figure(1,figsize=(13, 13))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "wordcloud_draw(wordcloud_data,'white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Classifier with real world Tweets\n",
    "### Set-up MongoDB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO_URL = 'mongodb://twitter-mongodb:27017/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_mongo(mongo_db, mongo_db_coll, return_cursor=False, criteria=None, projection=None):\n",
    "    # Optionally, use criteria and projection to limit the data that is\n",
    "    # returned - http://docs.mongodb.org/manual/reference/method/db.collection.find/\n",
    "    \n",
    "    # Connects to the MongoDB server running on\n",
    "    client = pymongo.MongoClient(MONGO_URL)\n",
    "    # Reference a particular collection in the database\n",
    "    db = client[mongo_db]\n",
    "    # Perform a bulk insert and return the IDs\n",
    "    coll = db[mongo_db_coll]\n",
    "    if criteria is None:\n",
    "        criteria = {}\n",
    "    if projection is None:\n",
    "        cursor = coll.find(criteria)\n",
    "    else:\n",
    "        cursor = coll.find(criteria, projection)\n",
    "    \n",
    "    # Returning a cursor is recommended for large amounts of data\n",
    "    if return_cursor:\n",
    "        return cursor\n",
    "    else:\n",
    "        return [ item for item in cursor ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define if trends should be used as tweets or trumps tweets\n",
    "# use either 'trump' or 'trends'\n",
    "database = 'trump'\n",
    "\n",
    "data = load_from_mongo(database, 'tweets')\n",
    "tweets_from_mongo = [tweet['text'] for tweet in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unwanted characters from Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [clean_tweet(tweet) for tweet in tweets_from_mongo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Tweets and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize tweets\n",
    "tokenized_tweets = [nltk.tokenize.word_tokenize(tweet.lower()) for tweet in tweets]\n",
    "\n",
    "# remove stopwords\n",
    "for tweet in tokenized_tweets:\n",
    "    remove_stopwords(tweet)\n",
    "\n",
    "# print available #nr of tweets to know range for next code snippet\n",
    "print(\"Available numbers of tweets: %s\" % len(tokenized_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Classifier with one of the fetched Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change tweet number to analyse different tweets that where fetched\n",
    "tweet_nr = 99\n",
    "\n",
    "trump_tweet = tokenized_tweets[tweet_nr]\n",
    "print(tweets[tweet_nr])\n",
    "print(\"Trump's Sentiment about the Tweet:\", classifier.classify(extract_features(trump_tweet)).upper())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
